<!DOCTYPE html>
<html lang="en">
  <head>
    <!-- Metadata of the Webpage -->
    <!-- Character-set Metadata -->
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <!-- Viewport Metadata -->
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <!-- Description Metadata-->
    <meta name="description" content="An advanced data engineering project building an analytics pipeline for Prague metro system data using Airflow, PostgreSQL, DuckDB, and Grafana." />
    <!-- Author Metadata -->
    <meta name="author" content="Tatum Good" />
    <!-- Keyword Metadata -->
    <meta
      name="keywords"
      content="Tatum Good, Data Engineering, Metro Pipeline, Apache Airflow, 
      DuckDB, PostgreSQL, Grafana Dashboards, Prague Metro, Data Warehouse, 
      Real-Time Analytics, Public Transit Data, Python, ETL"
    />
    <!-- Webpage Logo -->
    <link rel="shortcut icon" href="./assets/img/favicon.ico" />
    <!-- Webpage Title -->
    <title>Riding the Data Pipeline | Prague Metro Data Engineering Project</title>

    <!-- Import CSS: Main Stylesheet -->
    <link rel="stylesheet" href="./assets/css/link.css" />
    <link rel="stylesheet" type='text/css' href="https://cdn.jsdelivr.net/gh/devicons/devicon@latest/devicon.min.css" />
    <link rel="stylesheet" href="./assets/css/custom-icons.css" />
          
  </head>

  <body>
    <div id="particles-js" style="background-color:#f0b4ee;">
      <div class="header">
        <h1>
          <span class="site-title">Riding the Data Pipeline</span>
          <span class="site-description">Advanced Data Engineering Project Exploring Prague Metro Active Data</span>
        </h1>
        <div class="header-links">
          <a class="link" href="#overview" data-scroll>Description</a>
          <a class="link" href="#skills" data-scroll>Tools Used</a>
          <a class="link" href="#projects" data-scroll>Data Process</a>
        </div>
      </div>
    </div>

    <!-- About Section -->
    <section id="overview">
      <!-- User Introduction-->
      <div class="user-details" style="max-width: 1000px; margin: 0 auto;">
        <h1>Overview</h1>
        <p>
          This project focuses on designing and implementing a comprehensive data pipeline and analytics platform for a simulated metro system in Prague. 
          The core objective is to integrate and process multiple evolving data sources—from metro ridership and station information to rider feedback and sensor data—into a centralized data lake and warehouse. 
          These datasets will be used to build clear, actionable dashboards that empower metro operations and city planners to make informed, data-driven decisions. 
          Throughout the semester, the project grows in complexity as it incorporates new data types and addresses progressively deeper business questions related to ridership patterns, fare group behavior, train performance, and system efficiency.
          The technical stack includes batch data processing using Airflow orchestrated pipelines, PostgreSQL for metadata management, and Grafana for dashboard visualization, all maintained under strict version control and robust infrastructure.
        </p>
      </div>
      </section>

    <!-- Skills Section -->
    <section id="skills">
      <!-- User Specifics -->
      <div class="user" style="max-width: 1000px; margin: 0 auto;">
       <!--  <h1>Skills</h1> -->
        <!-- Technology Stack #1: Python -->
        <div class="tech">
          <h2>Skills and Tools Used</h2>
          <i class="devicon-postgresql-plain-wordmark colored"></i>
          <i class="devicon-mongodb-plain-wordmark colored"></i>
          <i class="devicon-apacheairflow-plain colored"></i>
          <img src="./assets/img/jpg/duckdb.png" alt="duckdb" class="custom-icon" />
          <img src="./assets/img/jpg/minio.png" alt="MinIO" class="custom-icon" />
          <i class="devicon-grafana-plain-wordmark colored"></i>
          
          
          <p>Python, with pandas and psycopg2, was used for data manipulation and database connectivity. 
            SQL and PostgreSQL served as the foundation for data warehousing and querying. Apache Airflow handled scheduling and timezone-aware orchestration of data pipelines. 
            Grafana provided interactive dashboards and time-series visualizations. 
            Cloud storage was managed through S3 buckets, with team collaboration supported via GitHub and Slack. </p>
        </div>

      </div>
    </section>
    <p style="font-size: 0.8rem; color: #fff; margin-top: 2rem;">
  Skills: PostgreSQL, SQL, pandas, MongoDB, psycopg2, MinIO, Grafana, Apache Airflow
</p>

    <!-- Projects Section -->
    <section id="projects">
      <div class="user-details">
        <h1>Data Process</h1>
      </div>

      <!-- User Project #1: metro pipeline -->
      <div class="user-projects">
        <div class="images-right">
          <picture>
            <source type="image/webp" srcset="./assets/img/webp/metro2.webp" alt="Personal Website" />
            <img alt="metro image" src="./assets/img/jpg/metro2.png" />
          </picture>
        </div>
        <div class="contents" style="text-align: center">
          <h3>Data Ingestion & Access</h3>
          <div>
            &nbsp;
            <img height="32" width="32" src="https://unpkg.com/simple-icons@3.4.0/icons/github.svg" />
            &nbsp;
            <img height="32" width="32" src="https://unpkg.com/simple-icons@3.4.0/icons/docker.svg" />
            &nbsp;
            <img height="32" width="32" src="https://unpkg.com/simple-icons@3.4.0/icons/airflow.svg" />
          </div>
          <p style="text-align: justify">
            The project involves accessing continuously updated relational databases containing core metro data such as stations, routes, passengers, and tap-in/tap-out events. 
            Access is secured through SSH tunneling and an infrastructure setup that facilitates smooth data extraction. 
            Daily snapshots are captured to enable batch processing, ensuring the data remains current without relying on real-time updates.
          </p>
          <a class="project-link" target="_blank" href="https://github.com/k-leonard/Kitada-Smalley_Good_Leonard_Research">Check it out!</a>
        </div>
      </div>

      <!-- User Project #2:  -->
      <div class="user-projects">
        <div class="images-left">
          <picture>
            <source type="image/webp" srcset="./assets/img/webp/metro1.webp" alt="Personal Website" />
            <img alt="Personal Website" src="./assets/img/jpg/metro1.png" />
          </picture>
        </div>
        <div class="contents-right" style="text-align: center">
          <h3>Data Transformation & Integration</h3>
          <img
            height="32"
            width="32"
            src="https://unpkg.com/simple-icons@3.4.0/icons/react.svg"
            style="filter: invert(73%) sepia(74%) saturate(1552%) hue-rotate(169deg) brightness(109%) contrast(97%)"
          />
          &nbsp;
          <img
            height="32"
            width="32"
            src="https://unpkg.com/simple-icons@3.4.0/icons/css3.svg"
            style="filter: invert(33%) sepia(29%) saturate(3844%) hue-rotate(184deg) brightness(90%) contrast(84%)"
          />
          &nbsp;
          <img height="32" width="32" src="https://unpkg.com/simple-icons@3.4.0/icons/github.svg" />
          <p style="text-align: justify">
            Airflow DAGs orchestrate the extraction, cleaning, and transformation of raw data into a unified and coherent data lake and warehouse. 
            Evolving data dimensions, including changing rider fare types and the addition of new stations, are managed carefully to maintain data integrity. 
            The data is enriched by establishing connections between related entities such as rides, routes, and fare groups, supporting multidimensional analysis.
          </p>
          <a class="project-link" target="_blank" href="https://github.com/AVS1508/Summer-2020-Project"
            >Check it out!</a
          >
        </div>
      </div>

      <!-- User Project #3: COVID-19 Tracker App -->
      <div class="user-projects">
        <div class="images-right">
          <picture>
            <source type="image/webp" srcset="./assets/img/webp/metro1.webp" alt="Personal Website" />
            <img alt="Personal Website" src="./assets/img/jpg/metro1.png" />
          </picture>
        </div>
        <div class="contents" style="text-align: center">
          <h3>Data Analysis & Visualization</h3>
          <div>
            <img
              height="32"
              width="32"
              src="https://unpkg.com/simple-icons@3.4.0/icons/javascript.svg"
              style="filter: invert(96%) sepia(47%) saturate(4151%) hue-rotate(338deg) brightness(100%) contrast(95%)"
            />
          </div>
          <p style="text-align: justify">
            Polished, user-friendly dashboards in Grafana provide answers to key business questions concerning ridership demand, travel times, and demographic usage patterns. 
            These dashboards offer clear insights to guide operational decisions like train allocation, schedule adjustments, and targeted service improvements. 
            Ongoing maintenance ensures that dashboard accuracy and functionality remain intact as new data and business questions develop over time.
          </p>
          <a class="project-link" target="_blank" href="https://www.adityavsingh.com/covid19-tracker-app">
            Check it out!
          </a>
        </div>
      </div>

      <!-- Use Project #4: Capstone -->
      <div class="user-projects">
        <div class="user-details">
          <h3>Results</h3>
          <p style="text-align: justify">
            This project deepened my practical understanding of data engineering and analytics at scale. 
            I gained hands-on experience designing modular, scalable data pipelines using industry-standard tools like Apache Airflow, PostgreSQL, and MinIO. 
            I learned how to structure and orchestrate batch workflows that accommodate evolving schemas and dependencies, ensuring our data warehouse remains accurate and reliable over time. 
            By building pipelines that adapt to incoming changes—such as new fare groups or added stations—I developed a stronger appreciation for maintaining data integrity in a dynamic environment.

            From a collaboration perspective, I learned the importance of version control, structured team communication, and clear division of responsibilities. 
            As the dataset evolved and the complexity of our business questions increased, our team had to remain agile, frequently refining DAG logic, SQL queries, and dashboard parameters. 
            This taught me to think not just like a data engineer or analyst, but like a systems architect—considering long-term scalability, maintainability, and usability. 
            Ultimately, this project helped bridge the gap between technical implementation and stakeholder impact, reinforcing the importance of transforming raw data into actionable insights for real-world decision-making.
          </p>
        </div>
      </div>
    </section>

    <footer class="footer">
      <p>&copy; Tatum Good, 2025</p>
    </footer>

    <!-- Import JS: Particles Theme -->
    <script src="https://cdn.jsdelivr.net/particles.js/2.0.0/particles.min.js"></script>
    <!-- Import JS: Sweet Scroll -->
    <script src="./assets/js/sweet-scroll.min.js"></script>
    <!-- Import JS: Google Analytics -->
    <script src="./assets/js/google-analytics.js"></script>
    <!-- Import JS: Main Script -->
    <script src="./assets/js/main.js"></script>
  </body>
</html>
